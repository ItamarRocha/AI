{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning class exercise list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_size, train_val_split = 0.1):\n",
    "    X = np.zeros((data_size, 3),dtype=np.float128)\n",
    "    y = np.zeros((data_size, 8),dtype=np.float128)\n",
    "    for i in range(data_size):\n",
    "        arr = np.random.randint(0, 2, 3) + np.random.uniform(-0.1,0.1, 3)\n",
    "        X[i] = np.round(arr,4)\n",
    "        y[i][int(round(arr[0]) * 4 + round(arr[1]) * 2+ round(arr[2]))] = 1\n",
    "    \n",
    "    val_split = round(data_size * (1 - train_val_split))\n",
    "\n",
    "    X_train, y_train = X[:val_split].T, y[:val_split].T\n",
    "    X_val, y_val = X[val_split:].T, y[val_split:].T\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(value):\n",
    "    expA = np.exp(value.T - np.max(value.T, axis=1, keepdims=True))\n",
    "    return (expA / expA.sum(axis=1, keepdims=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    return np.maximum(value, 0)\n",
    "\n",
    "def relu_derivative(value):\n",
    "    value[relu(value) <=0] = 0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes your class:\n",
    "            parameters : dictionary of parameters, which will store W and b through propagation.\n",
    "            cache : dictionary of cache, which will be responsible for storing A and Z during the propagation.\n",
    "            grads: dictionary of gradients, which will store all gradients computed during backprop.\n",
    "        \n",
    "        Args:\n",
    "            No arguments taken.\n",
    "        return:\n",
    "            No return.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "\n",
    "    def fit(self, X_train, y_train, hidden=relu, output=softmax):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            X_train = input data of shape (n_x, number_of_examples).\n",
    "            y_train = label vector of shape (n_y, number_of_examples).\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.m = X_train.shape[1]\n",
    "        self.hidden = hidden # function passed as argument to be used on hidden layers\n",
    "        self.output = output # function passed as argument to be used on output layers\n",
    "\n",
    "    def initialize_parameters(self, dims, adam_optimizer=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dims = dimensions of the network.\n",
    "            \n",
    "            Example:\n",
    "                dims = [3,3,8]\n",
    "                \n",
    "                A network with input size = 3, hidden layer = 3 and output layer = 8.\n",
    "                \n",
    "                The first dimension on the list must always be the length of each example.\n",
    "                The last dimension on the list must always be the length of each output example.\n",
    "                \n",
    "                In a case where X_train shape = (3, 4500) and y_train shape = (8, 4500), 4500 in\n",
    "                each shape represents the number of examples.\n",
    "                \n",
    "                dims = [3, 8]\n",
    "        Return:\n",
    "            parameters : a dictionary containing all weights and biases intialized\n",
    "                \n",
    "        \"\"\"\n",
    "        self.L = len(dims)\n",
    "        for l in range(1, self.L):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(dims[l], dims[l-1]) * 0.01\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((dims[l], 1))\n",
    "            if adam_optimizer:\n",
    "                self.v[\"VdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.v[\"Vdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "                self.s[\"SdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.s[\"Sdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "        return self.parameters\n",
    "    \n",
    "    def propagate(self, X):\n",
    "        \"\"\"\n",
    "        Does the forward propagation of the network\n",
    "        \"\"\"\n",
    "        A_prev = X\n",
    "        self.cache[f\"A{0}\"] = A_prev\n",
    "        for l in range(1, self.L):\n",
    "            \n",
    "            Z = np.dot(self.parameters[f\"W{l}\"], A_prev) + self.parameters[f\"b{l}\"]\n",
    "\n",
    "            if l == self.L - 1:\n",
    "                A = self.output(Z)\n",
    "            else:\n",
    "                A = self.hidden(Z)\n",
    "\n",
    "            self.cache[f\"Z{l}\"] = Z\n",
    "            self.cache[f\"A{l}\"] = A\n",
    "            \n",
    "            A_prev = A\n",
    "        \n",
    "        self.y_hat = A\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the value using the propagate function\n",
    "        \n",
    "        Args:\n",
    "            X : data to be used on prediction\n",
    "        Return:\n",
    "            y_hat : data predicted\n",
    "        \"\"\"\n",
    "        self.propagate(X)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def compute_cost(self):\n",
    "        pred = self.y_hat.T\n",
    "        real = self.y_train.T\n",
    "        n_samples = real.shape[0]\n",
    "        logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "        cost = np.sum(logp)/(n_samples)\n",
    "        return cost\n",
    "\n",
    "    def loss(self):\n",
    "        res = self.y_hat - self.y_train\n",
    "        return res\n",
    "\n",
    "    def backprop(self):\n",
    "        dA = self.loss()\n",
    "        \n",
    "        if self.output == sigmoid:\n",
    "            dZ = dA * sigmoid_derivative(self.cache[f\"Z{self.L - 1}\"])\n",
    "        elif self.output == softmax:\n",
    "            dZ = dA\n",
    "        else:\n",
    "            print(\"output activation not recognized\")\n",
    "\n",
    "        self.grads[f\"dW{self.L - 1}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{self.L - 2}\"].T))\n",
    "        self.grads[f\"db{self.L - 1}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        for l in reversed(range(1, self.L - 1)):\n",
    "            self.grads[f\"dA_prev{l}\"] = np.dot(self.parameters[f\"W{l + 1}\"].T,dZ)\n",
    "            dZ = self.grads[f\"dA_prev{l}\"] * relu_derivative(self.cache[f\"Z{l}\"])\n",
    "            self.grads[f\"dW{l}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{l - 1}\"].T))\n",
    "            self.grads[f\"db{l}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    def update_grads_adam(self, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \n",
    "            beta1 : exponentially weighted average, beta = 0.9 is approx 10 days ewa\n",
    "        \"\"\"\n",
    "        v_biasCorrected = {}\n",
    "        s_biasCorrected = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            # moving average of the gradients\n",
    "            self.v[f\"VdW{l}\"] = beta1 * self.v[f\"VdW{l}\"] + (1 - beta1)* self.grads[f\"dW{l}\"]\n",
    "            self.v[f\"Vdb{l}\"] = beta1 * self.v[f\"Vdb{l}\"] + (1 - beta1)* self.grads[f\"db{l}\"]\n",
    "\n",
    "            v_biasCorrected[f\"VdW{l}\"] = self.v[f\"VdW{l}\"]/(1 - beta1 ** t) # bias correction to the first updates\n",
    "            v_biasCorrected[f\"Vdb{l}\"] = self.v[f\"Vdb{l}\"]/(1 - beta1 ** t) # bias correction\n",
    "\n",
    "            self.s[f\"SdW{l}\"] = beta2 * self.s[f\"SdW{l}\"] + (1 - beta2) * np.square(self.grads[f\"dW{l}\"])\n",
    "            self.s[f\"Sdb{l}\"] = beta2 * self.s[f\"Sdb{l}\"] + (1 - beta2) * np.square(self.grads[f\"db{l}\"])\n",
    "                                                                                             \n",
    "            s_biasCorrected[f\"SdW{l}\"] = self.s[f\"SdW{l}\"]/(1 - beta2 ** t) # bias correction to the first updates\n",
    "            s_biasCorrected[f\"Sdb{l}\"] = self.s[f\"Sdb{l}\"]/(1 - beta2 ** t) # bias correction\n",
    "            \n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (v_biasCorrected[f\"VdW{l}\"])/(np.sqrt(s_biasCorrected[f\"SdW{l}\"]) + epsilon)\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (v_biasCorrected[f\"Vdb{l}\"])/(np.sqrt(s_biasCorrected[f\"Sdb{l}\"]) + epsilon)\n",
    "                                                                                               \n",
    "    def update_grads_gd(self, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            beta1 : exponentially weighted average, beta = 0.9 is approx 10 days ewa\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (self.grads[f\"dW{l}\"])\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (self.grads[f\"db{l}\"])\n",
    "\n",
    "    def train(self, dims, learning_rate = 0.01, iterations = 1000, adam_optimizer=False):\n",
    "        printing_interval = round(iterations * 0.01)\n",
    "        self.initialize_parameters(dims, adam_optimizer=adam_optimizer)\n",
    "        costs = []\n",
    "        for i in range(iterations):\n",
    "            self.propagate(self.X_train)\n",
    "            cost = self.compute_cost()\n",
    "            if i % printing_interval == 0:\n",
    "                print(f\"epoch {i} : {cost}\")\n",
    "            costs.append(cost)\n",
    "            self.backprop()\n",
    "            if adam_optimizer:\n",
    "                self.update_grads_adam(t=i+1, learning_rate=learning_rate)\n",
    "            else:\n",
    "                self.update_grads_gd(learning_rate = learning_rate)\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val , y_train, y_val = generate_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : 2.079432921486915\n",
      "epoch 4 : 1.8480015101304932\n",
      "epoch 8 : 1.3412846011476423\n",
      "epoch 12 : 0.7765311361562025\n",
      "epoch 16 : 0.3674470259640076\n",
      "epoch 20 : 0.13441789858068676\n",
      "epoch 24 : 0.04054525765860448\n",
      "epoch 28 : 0.012795110316442069\n",
      "epoch 32 : 0.004642810307760303\n",
      "epoch 36 : 0.0021778988203075143\n",
      "epoch 40 : 0.0012255661969152822\n",
      "epoch 44 : 0.0007819019325574763\n",
      "epoch 48 : 0.0005595426210197278\n",
      "epoch 52 : 0.0004418377397806554\n",
      "epoch 56 : 0.00037443409572433367\n",
      "epoch 60 : 0.00033206449542600537\n",
      "epoch 64 : 0.00030344206554337966\n",
      "epoch 68 : 0.00028341663074443345\n",
      "epoch 72 : 0.00026912736619573804\n",
      "epoch 76 : 0.0002585811634306752\n",
      "epoch 80 : 0.0002503400684648223\n",
      "epoch 84 : 0.00024347932429653676\n",
      "epoch 88 : 0.00023749168568411083\n",
      "epoch 92 : 0.0002321244540130208\n",
      "epoch 96 : 0.00022723295356956243\n",
      "epoch 100 : 0.0002227029779840512\n",
      "epoch 104 : 0.00021843432681013466\n",
      "epoch 108 : 0.00021434985983979113\n",
      "epoch 112 : 0.0002104010370335425\n",
      "epoch 116 : 0.0002065625511524302\n",
      "epoch 120 : 0.0002028218669632109\n",
      "epoch 124 : 0.00019917122304224022\n",
      "epoch 128 : 0.00019560432031095112\n",
      "epoch 132 : 0.00019211625130422002\n",
      "epoch 136 : 0.00018870373278433985\n",
      "epoch 140 : 0.00018536505614249224\n",
      "epoch 144 : 0.00018209926253176227\n",
      "epoch 148 : 0.00017890545276625044\n",
      "epoch 152 : 0.00017578239724426215\n",
      "epoch 156 : 0.00017272861762214942\n",
      "epoch 160 : 0.00016974243087303605\n",
      "epoch 164 : 0.00016682212389498806\n",
      "epoch 168 : 0.00016396605310734825\n",
      "epoch 172 : 0.00016117258478047484\n",
      "epoch 176 : 0.00015844020797049793\n",
      "epoch 180 : 0.0001557675921372114\n",
      "epoch 184 : 0.00015315351005232728\n",
      "epoch 188 : 0.00015059680791491183\n",
      "epoch 192 : 0.00014809635416103523\n",
      "epoch 196 : 0.00014565100156046593\n",
      "epoch 200 : 0.00014325962049748807\n",
      "epoch 204 : 0.00014092107214479693\n",
      "epoch 208 : 0.00013863418346476465\n",
      "epoch 212 : 0.0001363977577835668\n",
      "epoch 216 : 0.0001342106148528037\n",
      "epoch 220 : 0.00013207156251351148\n",
      "epoch 224 : 0.000129979425225319\n",
      "epoch 228 : 0.0001279330728501689\n",
      "epoch 232 : 0.0001259313528371114\n",
      "epoch 236 : 0.00012397315572697414\n",
      "epoch 240 : 0.00012205743002167472\n",
      "epoch 244 : 0.00012018308549777108\n",
      "epoch 248 : 0.00011834907981738637\n",
      "epoch 252 : 0.00011655440712025089\n",
      "epoch 256 : 0.00011479807764856251\n",
      "epoch 260 : 0.00011307911298507199\n",
      "epoch 264 : 0.00011139656712926372\n",
      "epoch 268 : 0.00010974952042981611\n",
      "epoch 272 : 0.00010813708014484493\n",
      "epoch 276 : 0.00010655836144669142\n",
      "epoch 280 : 0.00010501250266180486\n",
      "epoch 284 : 0.00010349868894367453\n",
      "epoch 288 : 0.0001020160992672047\n",
      "epoch 292 : 0.00010056394095699118\n",
      "epoch 296 : 9.914144486418492e-05\n",
      "epoch 300 : 9.77478592874579e-05\n",
      "epoch 304 : 9.63824555489987e-05\n",
      "epoch 308 : 9.504452072559451e-05\n",
      "epoch 312 : 9.373336176489062e-05\n",
      "epoch 316 : 9.244830855186019e-05\n",
      "epoch 320 : 9.118870943545024e-05\n",
      "epoch 324 : 8.995393078269727e-05\n",
      "epoch 328 : 8.874335653463278e-05\n",
      "epoch 332 : 8.755638790635533e-05\n",
      "epoch 336 : 8.639244357164766e-05\n",
      "epoch 340 : 8.525095644027295e-05\n",
      "epoch 344 : 8.413137632255394e-05\n",
      "epoch 348 : 8.303316840588333e-05\n",
      "epoch 352 : 8.195581710955035e-05\n",
      "epoch 356 : 8.089881315539362e-05\n",
      "epoch 360 : 7.986166477318909e-05\n",
      "epoch 364 : 7.884389518149802e-05\n",
      "epoch 368 : 7.784503946512374e-05\n",
      "epoch 372 : 7.686464638477966e-05\n",
      "epoch 376 : 7.590227983698778e-05\n",
      "epoch 380 : 7.495751152415328e-05\n",
      "epoch 384 : 7.40299265084691e-05\n",
      "epoch 388 : 7.311912281239516e-05\n",
      "epoch 392 : 7.222470841989981e-05\n",
      "epoch 396 : 7.134630008959807e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neural_network.train([X_train.shape[0],8, y_train.shape[0]], iterations=400, learning_rate=0.1, adam_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_network.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 0, 1, 5, 4, 0, 5, 4, 2, 6, 6, 6, 5, 5, 5, 4, 3, 1, 2, 7,\n",
       "       0, 3, 5, 4, 2, 1, 0, 4, 6, 4, 3, 5, 5, 1, 5, 1, 0, 7, 5, 1, 3, 3,\n",
       "       1, 2, 1, 2, 7, 4, 3, 3, 5, 4, 0, 5, 3, 5, 5, 6, 2, 5, 5, 7, 4, 4,\n",
       "       0, 4, 0, 1, 3, 4, 3, 6, 0, 1, 2, 6, 5, 4, 4, 4, 3, 4, 4, 3, 5, 1,\n",
       "       2, 7, 5, 3, 4, 5, 2, 1, 4, 6, 7, 3, 6, 6, 6, 0, 2, 6, 2, 2, 0, 3,\n",
       "       2, 1, 1, 0, 0, 0, 3, 3, 6, 1, 7, 2, 6, 7, 2, 5, 4, 4, 6, 3, 4, 2,\n",
       "       2, 5, 6, 7, 3, 0, 7, 4, 7, 1, 5, 5, 3, 2, 2, 3, 6, 7, 3, 5, 5, 7,\n",
       "       2, 2, 3, 6, 7, 5, 6, 2, 2, 2, 2, 2, 6, 0, 2, 5, 6, 2, 5, 5, 4, 5,\n",
       "       6, 3, 2, 6, 7, 0, 7, 6, 4, 1, 6, 0, 6, 4, 4, 1, 5, 0, 6, 0, 7, 0,\n",
       "       2, 0, 7, 5, 5, 7, 4, 5, 7, 7, 1, 6, 7, 0, 2, 2, 6, 3, 6, 1, 7, 4,\n",
       "       0, 3, 0, 7, 2, 6, 7, 6, 4, 2, 2, 3, 1, 4, 1, 5, 0, 5, 3, 5, 1, 6,\n",
       "       7, 2, 7, 6, 7, 1, 4, 4, 2, 1, 6, 5, 4, 3, 5, 0, 6, 5, 2, 4, 1, 7,\n",
       "       1, 6, 0, 2, 3, 7, 3, 5, 4, 0, 6, 4, 4, 5, 6, 5, 3, 4, 3, 3, 3, 5,\n",
       "       7, 5, 5, 2, 4, 5, 6, 4, 6, 0, 0, 4, 2, 6, 3, 3, 3, 4, 7, 6, 7, 2,\n",
       "       7, 1, 1, 2, 5, 7, 7, 5, 3, 0, 2, 6, 4, 7, 1, 2, 0, 7, 3, 2, 2, 5,\n",
       "       4, 7, 2, 0, 0, 0, 7, 5, 7, 0, 6, 3, 5, 6, 1, 7, 7, 3, 6, 2, 4, 6,\n",
       "       2, 3, 0, 5, 7, 4, 1, 0, 7, 2, 6, 1, 1, 3, 3, 6, 2, 3, 1, 3, 4, 5,\n",
       "       3, 6, 3, 5, 1, 5, 7, 7, 6, 5, 3, 7, 0, 7, 7, 3, 6, 1, 7, 5, 0, 6,\n",
       "       0, 3, 5, 7, 6, 3, 1, 7, 6, 2, 3, 5, 5, 1, 1, 0, 7, 7, 2, 5, 5, 6,\n",
       "       7, 0, 1, 7, 2, 3, 0, 0, 7, 0, 2, 6, 6, 7, 0, 5, 3, 5, 2, 1, 7, 1,\n",
       "       5, 4, 0, 7, 6, 5, 7, 2, 5, 5, 4, 7, 2, 3, 3, 2, 4, 6, 5, 1, 4, 3,\n",
       "       7, 1, 1, 0, 1, 1, 1, 3, 1, 4, 6, 5, 6, 1, 1, 4, 2, 7, 7, 2, 1, 0,\n",
       "       3, 1, 6, 1, 1, 2, 1, 7, 3, 6, 0, 7, 7, 3, 0, 0])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 0, 1, 5, 4, 0, 5, 4, 2, 6, 6, 6, 5, 5, 5, 4, 3, 1, 2, 7,\n",
       "       0, 3, 5, 4, 2, 1, 0, 4, 6, 4, 3, 5, 5, 1, 5, 1, 0, 7, 5, 1, 3, 3,\n",
       "       1, 2, 1, 2, 7, 4, 3, 3, 5, 4, 0, 5, 3, 5, 5, 6, 2, 5, 5, 7, 4, 4,\n",
       "       0, 4, 0, 1, 3, 4, 3, 6, 0, 1, 2, 6, 5, 4, 4, 4, 3, 4, 4, 3, 5, 1,\n",
       "       2, 7, 5, 3, 4, 5, 2, 1, 4, 6, 7, 3, 6, 6, 6, 0, 2, 6, 2, 2, 0, 3,\n",
       "       2, 1, 1, 0, 0, 0, 3, 3, 6, 1, 7, 2, 6, 7, 2, 5, 4, 4, 6, 3, 4, 2,\n",
       "       2, 5, 6, 7, 3, 0, 7, 4, 7, 1, 5, 5, 3, 2, 2, 3, 6, 7, 3, 5, 5, 7,\n",
       "       2, 2, 3, 6, 7, 5, 6, 2, 2, 2, 2, 2, 6, 0, 2, 5, 6, 2, 5, 5, 4, 5,\n",
       "       6, 3, 2, 6, 7, 0, 7, 6, 4, 1, 6, 0, 6, 4, 4, 1, 5, 0, 6, 0, 7, 0,\n",
       "       2, 0, 7, 5, 5, 7, 4, 5, 7, 7, 1, 6, 7, 0, 2, 2, 6, 3, 6, 1, 7, 4,\n",
       "       0, 3, 0, 7, 2, 6, 7, 6, 4, 2, 2, 3, 1, 4, 1, 5, 0, 5, 3, 5, 1, 6,\n",
       "       7, 2, 7, 6, 7, 1, 4, 4, 2, 1, 6, 5, 4, 3, 5, 0, 6, 5, 2, 4, 1, 7,\n",
       "       1, 6, 0, 2, 3, 7, 3, 5, 4, 0, 6, 4, 4, 5, 6, 5, 3, 4, 3, 3, 3, 5,\n",
       "       7, 5, 5, 2, 4, 5, 6, 4, 6, 0, 0, 4, 2, 6, 3, 3, 3, 4, 7, 6, 7, 2,\n",
       "       7, 1, 1, 2, 5, 7, 7, 5, 3, 0, 2, 6, 4, 7, 1, 2, 0, 7, 3, 2, 2, 5,\n",
       "       4, 7, 2, 0, 0, 0, 7, 5, 7, 0, 6, 3, 5, 6, 1, 7, 7, 3, 6, 2, 4, 6,\n",
       "       2, 3, 0, 5, 7, 4, 1, 0, 7, 2, 6, 1, 1, 3, 3, 6, 2, 3, 1, 3, 4, 5,\n",
       "       3, 6, 3, 5, 1, 5, 7, 7, 6, 5, 3, 7, 0, 7, 7, 3, 6, 1, 7, 5, 0, 6,\n",
       "       0, 3, 5, 7, 6, 3, 1, 7, 6, 2, 3, 5, 5, 1, 1, 0, 7, 7, 2, 5, 5, 6,\n",
       "       7, 0, 1, 7, 2, 3, 0, 0, 7, 0, 2, 6, 6, 7, 0, 5, 3, 5, 2, 1, 7, 1,\n",
       "       5, 4, 0, 7, 6, 5, 7, 2, 5, 5, 4, 7, 2, 3, 3, 2, 4, 6, 5, 1, 4, 3,\n",
       "       7, 1, 1, 0, 1, 1, 1, 3, 1, 4, 6, 5, 6, 1, 1, 4, 2, 7, 7, 2, 1, 0,\n",
       "       3, 1, 6, 1, 1, 2, 1, 7, 3, 6, 0, 7, 7, 3, 0, 0])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_val.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(y_pred.T, axis = 1), np.argmax(y_val.T, axis = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
