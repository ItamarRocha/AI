{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning class exercise list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_size, train_val_split = 0.1):\n",
    "    X = np.zeros((data_size, 3),dtype=np.float128)\n",
    "    y = np.zeros((data_size, 8),dtype=np.float128)\n",
    "    for i in range(data_size):\n",
    "        arr = np.random.randint(0, 2, 3) + np.random.uniform(-0.1,0.1, 3)\n",
    "        X[i] = np.round(arr,4)\n",
    "        y[i][int(round(arr[0]) * 4 + round(arr[1]) * 2+ round(arr[2]))] = 1\n",
    "    \n",
    "    val_split = round(data_size * (1 - train_val_split))\n",
    "\n",
    "    X_train, y_train = X[:val_split].T, y[:val_split].T\n",
    "    X_val, y_val = X[val_split:].T, y[val_split:].T\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(value):\n",
    "    expA = np.exp(value.T - np.max(value.T, axis=1, keepdims=True))\n",
    "    return (expA / expA.sum(axis=1, keepdims=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    return np.maximum(value, 0)\n",
    "\n",
    "def relu_derivative(value):\n",
    "    value[relu(value) <=0] = 0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Average\n",
    "$$\n",
    "V_{10} = \\frac{0.1\\theta_{10} + 0.9\\cdot0.1\\theta_{9} + 0.9^2\\cdot0.1\\theta_{8} + \\cdots + 0.9^8\\cdot0.1\\theta_{2} + 0.9^9\\cdot0.1\\theta_{1}}{10}\n",
    "$$\n",
    "* This is how we usually compute an average. Although, having to keep all theses values in memory is costly. One alternative way would be to compute $V_{10}$ taking into account only $V_9$ and $\\theta_{10}$.\n",
    "$$\n",
    "V_{10} = \\frac{9}{10}V_9 + \\frac{1}{10}\\theta_{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which can be modelled as:\n",
    "$$\n",
    "V_t = \\beta V_{t-1} + (1 - \\beta) \\theta_t\n",
    "$$\n",
    "Where $\\beta$ is calculled as $\\frac{t -1}{t}$  \n",
    "the time window can be calculed through having the value of $\\beta$ as $\\frac{1}{1 - \\beta}$ days/windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In early training, we have to do a bias correction in this averaging, in order to compensate the lack of data in previous time steps inserted on the time window. At time step 0, if working with a 0.9 $\\beta$, we will have a \n",
    "time window of 10, but our estimates will be biased and far from the real cause it will lack more terms that should make a parte of the exponentially weighted average. In order to fix this, we divide the $V$ value calculated by (1 - $\\beta^t$) which will \"normalize\" the initial values and wont affect the values the other ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_t = \\frac{V_t}{1 - \\beta^t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes your class:\n",
    "            parameters : dictionary of parameters, which will store W and b through propagation.\n",
    "            cache : dictionary of cache, which will be responsible for storing A and Z during the propagation.\n",
    "            grads: dictionary of gradients, which will store all gradients computed during backprop.\n",
    "        \n",
    "        Args:\n",
    "            No arguments taken.\n",
    "        return:\n",
    "            No return.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "\n",
    "    def fit(self, X_train, y_train, hidden=relu, output=softmax):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            X_train = input data of shape (n_x, number_of_examples).\n",
    "            y_train = label vector of shape (n_y, number_of_examples).\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.m = X_train.shape[1]\n",
    "        self.hidden = hidden # function passed as argument to be used on hidden layers\n",
    "        self.output = output # function passed as argument to be used on output layers\n",
    "\n",
    "    def initialize_parameters(self, dims, adam_optimizer=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dims = dimensions of the network.\n",
    "            \n",
    "            Example:\n",
    "                dims = [3,3,8]\n",
    "                \n",
    "                A network with input size = 3, hidden layer = 3 and output layer = 8.\n",
    "                \n",
    "                The first dimension on the list must always be the length of each example.\n",
    "                The last dimension on the list must always be the length of each output example.\n",
    "                \n",
    "                In a case where X_train shape = (3, 4500) and y_train shape = (8, 4500), 4500 in\n",
    "                each shape represents the number of examples.\n",
    "                \n",
    "                dims = [3, 8]\n",
    "        Return:\n",
    "            parameters : a dictionary containing all weights and biases intialized\n",
    "                \n",
    "        \"\"\"\n",
    "        self.L = len(dims)\n",
    "        for l in range(1, self.L):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(dims[l], dims[l-1]) * 0.01\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((dims[l], 1))\n",
    "            if adam_optimizer:\n",
    "                self.v[\"VdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.v[\"Vdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "                self.s[\"SdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.s[\"Sdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "        return self.parameters\n",
    "    \n",
    "    def propagate(self, X):\n",
    "        \"\"\"\n",
    "        Does the forward propagation of the network\n",
    "        \"\"\"\n",
    "        A_prev = X\n",
    "        self.cache[f\"A{0}\"] = A_prev\n",
    "        for l in range(1, self.L):\n",
    "            \n",
    "            Z = np.dot(self.parameters[f\"W{l}\"], A_prev) + self.parameters[f\"b{l}\"]\n",
    "\n",
    "            if l == self.L - 1:\n",
    "                A = self.output(Z)\n",
    "            else:\n",
    "                A = self.hidden(Z)\n",
    "\n",
    "            self.cache[f\"Z{l}\"] = Z\n",
    "            self.cache[f\"A{l}\"] = A\n",
    "            \n",
    "            A_prev = A\n",
    "        \n",
    "        self.y_hat = A\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the value using the propagate function\n",
    "        \n",
    "        Args:\n",
    "            X : data to be used on prediction\n",
    "        Return:\n",
    "            y_hat : data predicted\n",
    "        \"\"\"\n",
    "        self.propagate(X)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def compute_cost(self):\n",
    "        pred = self.y_hat.T\n",
    "        real = self.y_train.T\n",
    "        n_samples = real.shape[0]\n",
    "        logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "        cost = np.sum(logp)/(n_samples)\n",
    "        return cost\n",
    "\n",
    "    def loss(self):\n",
    "        res = self.y_hat - self.y_train\n",
    "        return res\n",
    "\n",
    "    def backprop(self):\n",
    "        dA = self.loss()\n",
    "        \n",
    "        if self.output == sigmoid:\n",
    "            dZ = dA * sigmoid_derivative(self.cache[f\"Z{self.L - 1}\"])\n",
    "        elif self.output == softmax:\n",
    "            dZ = dA\n",
    "        else:\n",
    "            print(\"output activation not recognized\")\n",
    "\n",
    "        self.grads[f\"dW{self.L - 1}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{self.L - 2}\"].T))\n",
    "        self.grads[f\"db{self.L - 1}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        for l in reversed(range(1, self.L - 1)):\n",
    "            self.grads[f\"dA_prev{l}\"] = np.dot(self.parameters[f\"W{l + 1}\"].T,dZ)\n",
    "            dZ = self.grads[f\"dA_prev{l}\"] * relu_derivative(self.cache[f\"Z{l}\"])\n",
    "            self.grads[f\"dW{l}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{l - 1}\"].T))\n",
    "            self.grads[f\"db{l}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    def update_grads_adam(self, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \n",
    "            beta1 : exponentially weighted average, beta = 0.9 is approx 10 days ewa\n",
    "        \"\"\"\n",
    "        v_biasCorrected = {}\n",
    "        s_biasCorrected = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            # moving average of the gradients\n",
    "            self.v[f\"VdW{l}\"] = beta1 * self.v[f\"VdW{l}\"] + (1 - beta1)* self.grads[f\"dW{l}\"]\n",
    "            self.v[f\"Vdb{l}\"] = beta1 * self.v[f\"Vdb{l}\"] + (1 - beta1)* self.grads[f\"db{l}\"]\n",
    "\n",
    "            v_biasCorrected[f\"VdW{l}\"] = self.v[f\"VdW{l}\"]/(1 - beta1 ** t) # bias correction to the first updates\n",
    "            v_biasCorrected[f\"Vdb{l}\"] = self.v[f\"Vdb{l}\"]/(1 - beta1 ** t) # bias correction\n",
    "\n",
    "            self.s[f\"SdW{l}\"] = beta2 * self.s[f\"SdW{l}\"] + (1 - beta2) * np.square(self.grads[f\"dW{l}\"])\n",
    "            self.s[f\"Sdb{l}\"] = beta2 * self.s[f\"Sdb{l}\"] + (1 - beta2) * np.square(self.grads[f\"db{l}\"])\n",
    "                                                                                             \n",
    "            s_biasCorrected[f\"SdW{l}\"] = self.s[f\"SdW{l}\"]/(1 - beta2 ** t) # bias correction to the first updates\n",
    "            s_biasCorrected[f\"Sdb{l}\"] = self.s[f\"Sdb{l}\"]/(1 - beta2 ** t) # bias correction\n",
    "            \n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (v_biasCorrected[f\"VdW{l}\"])/(np.sqrt(s_biasCorrected[f\"SdW{l}\"]) + epsilon)\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (v_biasCorrected[f\"Vdb{l}\"])/(np.sqrt(s_biasCorrected[f\"Sdb{l}\"]) + epsilon)\n",
    "                                                                                               \n",
    "    def update_grads_gd(self, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            beta1 : exponentially weighted average, beta = 0.9 is approx 10 days ewa\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (self.grads[f\"dW{l}\"])\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (self.grads[f\"db{l}\"])\n",
    "\n",
    "    def train(self, dims, learning_rate = 0.01, iterations = 1000, adam_optimizer=False):\n",
    "        printing_interval = round(iterations * 0.01)\n",
    "        self.initialize_parameters(dims, adam_optimizer=adam_optimizer)\n",
    "        costs = []\n",
    "        for i in range(iterations):\n",
    "            self.propagate(self.X_train)\n",
    "            cost = self.compute_cost()\n",
    "            if i % printing_interval == 0:\n",
    "                print(f\"epoch {i} : {cost}\")\n",
    "            costs.append(cost)\n",
    "            self.backprop()\n",
    "            if adam_optimizer:\n",
    "                self.update_grads_adam(t=i+1, learning_rate=learning_rate)\n",
    "            else:\n",
    "                self.update_grads_gd(learning_rate = learning_rate)\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val , y_train, y_val = generate_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : 2.079378997026494\n",
      "epoch 1 : 2.0634085099998627\n",
      "epoch 2 : 2.0269471044285474\n",
      "epoch 3 : 1.9783905668382245\n",
      "epoch 4 : 1.906547192523324\n",
      "epoch 5 : 1.8251136918501247\n",
      "epoch 6 : 1.7337501638656416\n",
      "epoch 7 : 1.6242553708621703\n",
      "epoch 8 : 1.4946937545374974\n",
      "epoch 9 : 1.3592004315190338\n",
      "epoch 10 : 1.2262535659747207\n",
      "epoch 11 : 1.0925657830476907\n",
      "epoch 12 : 0.9571552053515843\n",
      "epoch 13 : 0.8340876405558112\n",
      "epoch 14 : 0.7325200155472833\n",
      "epoch 15 : 0.6373060989403112\n",
      "epoch 16 : 0.5465024988092766\n",
      "epoch 17 : 0.470396478621391\n",
      "epoch 18 : 0.40801532563394505\n",
      "epoch 19 : 0.35185609881697805\n",
      "epoch 20 : 0.30361063507080455\n",
      "epoch 21 : 0.2648008285363894\n",
      "epoch 22 : 0.22565604567141678\n",
      "epoch 23 : 0.19236904189067672\n",
      "epoch 24 : 0.1661406612643288\n",
      "epoch 25 : 0.14200771381491278\n",
      "epoch 26 : 0.1223330968652099\n",
      "epoch 27 : 0.10667281247068897\n",
      "epoch 28 : 0.09185571038456486\n",
      "epoch 29 : 0.0796445435868532\n",
      "epoch 30 : 0.0698767353098965\n",
      "epoch 31 : 0.06130242126201424\n",
      "epoch 32 : 0.05444047782673452\n",
      "epoch 33 : 0.04849447307729679\n",
      "epoch 34 : 0.04316313513680735\n",
      "epoch 35 : 0.03888100704118784\n",
      "epoch 36 : 0.03503646391534055\n",
      "epoch 37 : 0.031540607768389406\n",
      "epoch 38 : 0.02872093085885445\n",
      "epoch 39 : 0.026247009054724015\n",
      "epoch 40 : 0.023920034109642815\n",
      "epoch 41 : 0.02194080071275323\n",
      "epoch 42 : 0.02021024804100069\n",
      "epoch 43 : 0.01858244783671909\n",
      "epoch 44 : 0.017139833748144595\n",
      "epoch 45 : 0.015942950431656937\n",
      "epoch 46 : 0.014894316883746268\n",
      "epoch 47 : 0.013922501380546107\n",
      "epoch 48 : 0.013056744306625985\n",
      "epoch 49 : 0.012302891551431494\n",
      "epoch 50 : 0.011619486072959132\n",
      "epoch 51 : 0.010990857758016644\n",
      "epoch 52 : 0.010438015939596746\n",
      "epoch 53 : 0.009965732816023759\n",
      "epoch 54 : 0.009540049729858228\n",
      "epoch 55 : 0.009129511632962953\n",
      "epoch 56 : 0.008739991411529553\n",
      "epoch 57 : 0.008390855767993107\n",
      "epoch 58 : 0.00808189657999369\n",
      "epoch 59 : 0.007798354190328504\n",
      "epoch 60 : 0.0075321965982189125\n",
      "epoch 61 : 0.007286879516936211\n",
      "epoch 62 : 0.007065506640002719\n",
      "epoch 63 : 0.006862116847467383\n",
      "epoch 64 : 0.00666735053492\n",
      "epoch 65 : 0.006478871988697202\n",
      "epoch 66 : 0.006301633891687927\n",
      "epoch 67 : 0.006139441757683136\n",
      "epoch 68 : 0.005990247267140283\n",
      "epoch 69 : 0.0058494133299945095\n",
      "epoch 70 : 0.005714665791234127\n",
      "epoch 71 : 0.005586829070620506\n",
      "epoch 72 : 0.005466907402008466\n",
      "epoch 73 : 0.005353864410465499\n",
      "epoch 74 : 0.005245592383256834\n",
      "epoch 75 : 0.005141188220905523\n",
      "epoch 76 : 0.005041383628760986\n",
      "epoch 77 : 0.0049469160377914306\n",
      "epoch 78 : 0.004857208449311727\n",
      "epoch 79 : 0.004770830311847657\n",
      "epoch 80 : 0.0046868343222343095\n",
      "epoch 81 : 0.004605374153457739\n",
      "epoch 82 : 0.004527129579051611\n",
      "epoch 83 : 0.004452357820786551\n",
      "epoch 84 : 0.004380602425547447\n",
      "epoch 85 : 0.0043111984129327015\n",
      "epoch 86 : 0.004243855703107282\n",
      "epoch 87 : 0.004178671104875123\n",
      "epoch 88 : 0.004115707574423312\n",
      "epoch 89 : 0.004054740167070015\n",
      "epoch 90 : 0.003995411210499267\n",
      "epoch 91 : 0.003937537590395958\n",
      "epoch 92 : 0.0038811860270781292\n",
      "epoch 93 : 0.0038264675484358376\n",
      "epoch 94 : 0.003773318963844026\n",
      "epoch 95 : 0.0037215011053540023\n",
      "epoch 96 : 0.003670786208917902\n",
      "epoch 97 : 0.003621103067119479\n",
      "epoch 98 : 0.0035725090106603955\n",
      "epoch 99 : 0.0035250471128788576\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neural_network.train([X_train.shape[0],8, y_train.shape[0]], iterations=100, learning_rate=0.1, adam_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_network.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 7, 2, 7, 2, 1, 1, 6, 4, 0, 2, 4, 1, 4, 7, 7, 0, 7, 4, 4, 7,\n",
       "       0, 6, 6, 2, 3, 7, 2, 4, 6, 0, 1, 1, 1, 4, 6, 0, 3, 1, 7, 1, 1, 4,\n",
       "       2, 4, 0, 7, 1, 2, 7, 0, 0, 2, 0, 7, 5, 2, 2, 3, 7, 4, 7, 6, 5, 1,\n",
       "       0, 4, 4, 0, 6, 2, 6, 4, 7, 2, 2, 4, 6, 3, 6, 5, 3, 4, 4, 1, 5, 1,\n",
       "       1, 1, 6, 0, 3, 4, 4, 7, 7, 7, 2, 3, 1, 4, 6, 0, 5, 4, 1, 4, 1, 7,\n",
       "       2, 1, 1, 5, 1, 2, 2, 7, 4, 1, 6, 7, 2, 0, 6, 1, 0, 4, 4, 2, 2, 3,\n",
       "       4, 4, 6, 4, 1, 1, 7, 5, 1, 7, 2, 7, 7, 5, 0, 5, 4, 3, 1, 7, 6, 6,\n",
       "       2, 2, 6, 5, 7, 6, 0, 3, 6, 2, 5, 3, 2, 5, 2, 5, 4, 1, 3, 0, 4, 3,\n",
       "       2, 7, 7, 1, 2, 4, 3, 6, 5, 3, 0, 7, 4, 2, 7, 4, 6, 3, 2, 7, 5, 6,\n",
       "       6, 5, 4, 6, 7, 7, 5, 7, 6, 5, 5, 3, 0, 5, 1, 5, 6, 5, 2, 1, 0, 1,\n",
       "       0, 7, 3, 0, 2, 6, 5, 7, 5, 4, 2, 0, 2, 5, 4, 0, 2, 1, 1, 3, 6, 5,\n",
       "       3, 3, 5, 5, 6, 7, 5, 3, 1, 7, 6, 2, 0, 6, 2, 0, 7, 3, 4, 6, 4, 7,\n",
       "       1, 0, 3, 4, 6, 7, 0, 6, 7, 2, 0, 0, 1, 4, 5, 4, 4, 1, 7, 3, 2, 4,\n",
       "       6, 7, 4, 7, 2, 2, 3, 2, 6, 6, 7, 3, 0, 0, 0, 2, 4, 6, 4, 0, 0, 7,\n",
       "       7, 0, 5, 3, 1, 5, 4, 6, 2, 0, 5, 3, 5, 3, 5, 2, 0, 6, 0, 6, 1, 6,\n",
       "       5, 3, 6, 1, 7, 2, 2, 4, 4, 1, 5, 7, 0, 2, 7, 4, 1, 6, 0, 0, 4, 7,\n",
       "       3, 3, 4, 1, 6, 1, 4, 4, 1, 7, 3, 6, 2, 1, 7, 2, 2, 7, 6, 0, 4, 7,\n",
       "       6, 0, 7, 1, 7, 6, 3, 3, 5, 7, 4, 4, 4, 2, 5, 4, 1, 4, 2, 0, 1, 4,\n",
       "       3, 5, 0, 3, 5, 2, 7, 0, 2, 2, 2, 7, 7, 0, 1, 5, 1, 0, 3, 0, 7, 4,\n",
       "       7, 2, 0, 7, 7, 4, 1, 4, 0, 6, 6, 5, 7, 4, 1, 5, 1, 0, 7, 2, 1, 6,\n",
       "       3, 1, 7, 7, 5, 3, 6, 1, 0, 5, 5, 7, 4, 6, 1, 1, 0, 4, 2, 1, 0, 4,\n",
       "       6, 3, 7, 6, 1, 3, 4, 2, 4, 5, 5, 0, 3, 0, 2, 1, 7, 5, 6, 3, 3, 4,\n",
       "       1, 6, 1, 0, 6, 3, 4, 3, 5, 4, 3, 2, 7, 5, 6, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 7, 2, 7, 2, 1, 1, 6, 4, 0, 2, 4, 1, 4, 7, 7, 0, 7, 4, 4, 7,\n",
       "       0, 6, 6, 2, 3, 7, 2, 4, 6, 0, 1, 1, 1, 4, 6, 0, 3, 1, 7, 1, 1, 4,\n",
       "       2, 4, 0, 7, 1, 2, 7, 0, 0, 2, 0, 7, 5, 2, 2, 3, 7, 4, 7, 6, 5, 1,\n",
       "       0, 4, 4, 0, 6, 2, 6, 4, 7, 2, 2, 4, 6, 3, 6, 5, 3, 4, 4, 1, 5, 1,\n",
       "       1, 1, 6, 0, 3, 4, 4, 7, 7, 7, 2, 3, 1, 4, 6, 0, 5, 4, 1, 4, 1, 7,\n",
       "       2, 1, 1, 5, 1, 2, 2, 7, 4, 1, 6, 7, 2, 0, 6, 1, 0, 4, 4, 2, 2, 3,\n",
       "       4, 4, 6, 4, 1, 1, 7, 5, 1, 7, 2, 7, 7, 5, 0, 5, 4, 3, 1, 7, 6, 6,\n",
       "       2, 2, 6, 5, 7, 6, 0, 3, 6, 2, 5, 3, 2, 5, 2, 5, 4, 1, 3, 0, 4, 3,\n",
       "       2, 7, 7, 1, 2, 4, 3, 6, 5, 3, 0, 7, 4, 2, 7, 4, 6, 3, 2, 7, 5, 6,\n",
       "       6, 5, 4, 6, 7, 7, 5, 7, 6, 5, 5, 3, 0, 5, 1, 5, 6, 5, 2, 1, 0, 1,\n",
       "       0, 7, 3, 0, 2, 6, 5, 7, 5, 4, 2, 0, 2, 5, 4, 0, 2, 1, 1, 3, 6, 5,\n",
       "       3, 3, 5, 5, 6, 7, 5, 3, 1, 7, 6, 2, 0, 6, 2, 0, 7, 3, 4, 6, 4, 7,\n",
       "       1, 0, 3, 4, 6, 7, 0, 6, 7, 2, 0, 0, 1, 4, 5, 4, 4, 1, 7, 3, 2, 4,\n",
       "       6, 7, 4, 7, 2, 2, 3, 2, 6, 6, 7, 3, 0, 0, 0, 2, 4, 6, 4, 0, 0, 7,\n",
       "       7, 0, 5, 3, 1, 5, 4, 6, 2, 0, 5, 3, 5, 3, 5, 2, 0, 6, 0, 6, 1, 6,\n",
       "       5, 3, 6, 1, 7, 2, 2, 4, 4, 1, 5, 7, 0, 2, 7, 4, 1, 6, 0, 0, 4, 7,\n",
       "       3, 3, 4, 1, 6, 1, 4, 4, 1, 7, 3, 6, 2, 1, 7, 2, 2, 7, 6, 0, 4, 7,\n",
       "       6, 0, 7, 1, 7, 6, 3, 3, 5, 7, 4, 4, 4, 2, 5, 4, 1, 4, 2, 0, 1, 4,\n",
       "       3, 5, 0, 3, 5, 2, 7, 0, 2, 2, 2, 7, 7, 0, 1, 5, 1, 0, 3, 0, 7, 4,\n",
       "       7, 2, 0, 7, 7, 4, 1, 4, 0, 6, 6, 5, 7, 4, 1, 5, 1, 0, 7, 2, 1, 6,\n",
       "       3, 1, 7, 7, 5, 3, 6, 1, 0, 5, 5, 7, 4, 6, 1, 1, 0, 4, 2, 1, 0, 4,\n",
       "       6, 3, 7, 6, 1, 3, 4, 2, 4, 5, 5, 0, 3, 0, 2, 1, 7, 5, 6, 3, 3, 4,\n",
       "       1, 6, 1, 0, 6, 3, 4, 3, 5, 4, 3, 2, 7, 5, 6, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_val.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(y_pred.T, axis = 1), np.argmax(y_val.T, axis = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
