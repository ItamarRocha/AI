{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning class exercise list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_size, train_val_split = 0.1):\n",
    "    X = np.zeros((data_size, 3),dtype=np.float128)\n",
    "    y = np.zeros((data_size, 8),dtype=np.float128)\n",
    "    for i in range(data_size):\n",
    "        arr = np.random.randint(0, 2, 3) + np.random.uniform(-0.1,0.1, 3)\n",
    "        X[i] = np.round(arr,4)\n",
    "        y[i][int(round(arr[0]) * 4 + round(arr[1]) * 2+ round(arr[2]))] = 1\n",
    "    \n",
    "    val_split = round(data_size * (1 - train_val_split))\n",
    "\n",
    "    X_train, y_train = X[:val_split].T, y[:val_split].T\n",
    "    X_val, y_val = X[val_split:].T, y[val_split:].T\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value):\n",
    "    return 1/(1 + np.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(value):\n",
    "    expA = np.exp(value.T - np.max(value.T, axis=1, keepdims=True))\n",
    "    return (expA / expA.sum(axis=1, keepdims=True)).T\n",
    "\n",
    "def softmax_derivative(value):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(value):\n",
    "    return np.maximum(value, 0)\n",
    "\n",
    "def relu_derivative(value):\n",
    "    value[relu(value) <=0] = 0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Average\n",
    "$$\n",
    "V_{10} = \\frac{0.1\\theta_{10} + 0.9\\cdot0.1\\theta_{9} + 0.9^2\\cdot0.1\\theta_{8} + \\cdots + 0.9^8\\cdot0.1\\theta_{2} + 0.9^9\\cdot0.1\\theta_{1}}{10}\n",
    "$$\n",
    "* This is how we usually compute an average. Although, having to keep all theses values in memory is costly. One alternative way would be to compute $V_{10}$ taking into account only $V_9$ and $\\theta_{10}$.\n",
    "$$\n",
    "V_{10} = \\frac{9}{10}V_9 + \\frac{1}{10}\\theta_{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which can be modelled as:\n",
    "$$\n",
    "V_t = \\beta V_{t-1} + (1 - \\beta) \\theta_t\n",
    "$$\n",
    "Where $\\beta$ is calculled as $\\frac{t -1}{t}$  \n",
    "the time window can be calculed through having the value of $\\beta$ as $\\frac{1}{1 - \\beta}$ days/windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In early training, we have to do a bias correction in this averaging, in order to compensate the lack of data in previous time steps inserted on the time window. At time step 0, if working with a 0.9 $\\beta$, we will have a \n",
    "time window of 10, but our estimates will be biased and far from the real cause it will lack more terms that should make a parte of the exponentially weighted average. In order to fix this, we divide the $V$ value calculated by (1 - $\\beta^t$) which will \"normalize\" the initial values and wont affect the values the other ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_t = \\frac{V_t}{1 - \\beta^t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes your class:\n",
    "            parameters : dictionary of parameters, which will store W and b through propagation.\n",
    "            cache : dictionary of cache, which will be responsible for storing A and Z during the propagation.\n",
    "            grads: dictionary of gradients, which will store all gradients computed during backprop.\n",
    "            v : dictionary with momentum ewa estimates\n",
    "            s : dictionary with RMSprop ewa estimates\n",
    "        Args:\n",
    "            No arguments taken.\n",
    "        return:\n",
    "            No return.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "\n",
    "    def fit(self, X_train, y_train, hidden=relu, output=softmax):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            X_train = input data of shape (n_x, number_of_examples).\n",
    "            y_train = label vector of shape (n_y, number_of_examples).\n",
    "            hidden : passed as argument the function used on the hidden layers\n",
    "            output : function used on output layer\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.m = X_train.shape[1]\n",
    "        self.hidden = hidden # function passed as argument to be used on hidden layers\n",
    "        self.output = output # function passed as argument to be used on output layers\n",
    "        \n",
    "        if self.output == sigmoid:\n",
    "            self.output_derivative = sigmoid_derivative\n",
    "        elif self.output == softmax:\n",
    "            self.output_derivative = softmax_derivative\n",
    "        else:\n",
    "            print(\"output activation not recognized\")\n",
    "            return -1\n",
    "        \n",
    "        if self.hidden == relu:\n",
    "            self.hidden_derivative = relu_derivative\n",
    "        elif self.hidden == sigmoid:\n",
    "            self.hidden_derivative = sigmoid_derivative\n",
    "        else:\n",
    "            print(\"hidden activation not recognized\")\n",
    "            return -1\n",
    "    \n",
    "    def initialize_parameters(self, dims, adam_optimizer=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dims = dimensions of the network.\n",
    "            \n",
    "            Example:\n",
    "                dims = [3,3,8]\n",
    "                \n",
    "                A network with input size = 3, hidden layer = 3 and output layer = 8.\n",
    "                \n",
    "                The first dimension on the list must always be the length of each example.\n",
    "                The last dimension on the list must always be the length of each output example.\n",
    "                \n",
    "                In a case where X_train shape = (3, 4500) and y_train shape = (8, 4500), 4500 in\n",
    "                each shape represents the number of examples.\n",
    "                \n",
    "                dims = [3, 8]\n",
    "        Return:\n",
    "            parameters : a dictionary containing all weights and biases intialized\n",
    "                \n",
    "        \"\"\"\n",
    "        self.L = len(dims)\n",
    "        for l in range(1, self.L):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(dims[l], dims[l-1]) * 0.01\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((dims[l], 1))\n",
    "            if adam_optimizer:\n",
    "                self.v[\"VdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.v[\"Vdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "                self.s[\"SdW\" + str(l)] = np.zeros((dims[l], dims[l-1]))\n",
    "                self.s[\"Sdb\" + str(l)] = np.zeros((dims[l], 1))\n",
    "        return self.parameters\n",
    "    \n",
    "    def propagate(self, X):\n",
    "        \"\"\"\n",
    "        Does the forward propagation of the network\n",
    "        \"\"\"\n",
    "        A_prev = X\n",
    "        self.cache[f\"A{0}\"] = A_prev\n",
    "        for l in range(1, self.L):\n",
    "            \n",
    "            Z = np.dot(self.parameters[f\"W{l}\"], A_prev) + self.parameters[f\"b{l}\"]\n",
    "\n",
    "            if l == self.L - 1:\n",
    "                A = self.output(Z)\n",
    "            else:\n",
    "                A = self.hidden(Z)\n",
    "\n",
    "            self.cache[f\"Z{l}\"] = Z\n",
    "            self.cache[f\"A{l}\"] = A\n",
    "            \n",
    "            A_prev = A\n",
    "        \n",
    "        self.y_hat = A\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the value using the propagate function\n",
    "        \n",
    "        Args:\n",
    "            X : data to be used on prediction\n",
    "        Return:\n",
    "            y_hat : data predicted\n",
    "        \"\"\"\n",
    "        self.propagate(X)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def compute_cost(self):\n",
    "        pred = self.y_hat.T\n",
    "        real = self.y_train.T\n",
    "        n_samples = real.shape[0]\n",
    "        logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "        cost = np.sum(logp)/(n_samples)\n",
    "        return cost\n",
    "\n",
    "    def loss(self):\n",
    "        res = self.y_hat - self.y_train\n",
    "        return res\n",
    "\n",
    "    def backprop(self):\n",
    "        dA = self.loss()\n",
    "        \n",
    "        dZ = dA * self.output_derivative(self.cache[f\"Z{self.L - 1}\"])\n",
    "        \n",
    "        self.grads[f\"dW{self.L - 1}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{self.L - 2}\"].T))\n",
    "        self.grads[f\"db{self.L - 1}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        \n",
    "        for l in reversed(range(1, self.L - 1)):\n",
    "            self.grads[f\"dA_prev{l}\"] = np.dot(self.parameters[f\"W{l + 1}\"].T,dZ)\n",
    "            dZ = self.grads[f\"dA_prev{l}\"] * self.hidden_derivative(self.cache[f\"Z{l}\"])\n",
    "            self.grads[f\"dW{l}\"] = 1/self.m * (np.dot(dZ, self.cache[f\"A{l - 1}\"].T))\n",
    "            self.grads[f\"db{l}\"] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    def update_grads_adam(self, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        \"\"\"\n",
    "        ADAM -> Adaptive Moment estimation\n",
    "        Args:\n",
    "            t : epoch number\n",
    "            learning_rate : learning rate chosed to upgrade weights\n",
    "            beta1 : exponentially weighted average used on v (momentum), beta1 = 0.9 (recommended on paper) is approx 10 days ewa\n",
    "            beta1 : exponentially weighted average used on s (RMSprop), beta2 = 0.999 (recommended on paper)\n",
    "            epsilon : term to prevent division by zero\n",
    "        \"\"\"\n",
    "        \n",
    "        v_biasCorrected = {}\n",
    "        s_biasCorrected = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        for l in reversed(range(1, self.L)):\n",
    "            # moving average of the gradients\n",
    "            self.v[f\"VdW{l}\"] = beta1 * self.v[f\"VdW{l}\"] + (1 - beta1)* self.grads[f\"dW{l}\"]\n",
    "            self.v[f\"Vdb{l}\"] = beta1 * self.v[f\"Vdb{l}\"] + (1 - beta1)* self.grads[f\"db{l}\"]\n",
    "\n",
    "            v_biasCorrected[f\"VdW{l}\"] = self.v[f\"VdW{l}\"]/(1 - beta1 ** t) # bias correction to the first updates\n",
    "            v_biasCorrected[f\"Vdb{l}\"] = self.v[f\"Vdb{l}\"]/(1 - beta1 ** t) # bias correction\n",
    "\n",
    "            self.s[f\"SdW{l}\"] = beta2 * self.s[f\"SdW{l}\"] + (1 - beta2) * np.square(self.grads[f\"dW{l}\"])\n",
    "            self.s[f\"Sdb{l}\"] = beta2 * self.s[f\"Sdb{l}\"] + (1 - beta2) * np.square(self.grads[f\"db{l}\"])\n",
    "                                                                                             \n",
    "            s_biasCorrected[f\"SdW{l}\"] = self.s[f\"SdW{l}\"]/(1 - beta2 ** t) # bias correction to the first updates\n",
    "            s_biasCorrected[f\"Sdb{l}\"] = self.s[f\"Sdb{l}\"]/(1 - beta2 ** t) # bias correction\n",
    "            \n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (v_biasCorrected[f\"VdW{l}\"])/(np.sqrt(s_biasCorrected[f\"SdW{l}\"]) + epsilon)\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (v_biasCorrected[f\"Vdb{l}\"])/(np.sqrt(s_biasCorrected[f\"Sdb{l}\"]) + epsilon)\n",
    "                                                                                               \n",
    "    def update_grads_gd(self, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate : learning rate chosed to upgrade weights\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            self.parameters[f\"W{l}\"] -= self.learning_rate * (self.grads[f\"dW{l}\"])\n",
    "            self.parameters[f\"b{l}\"] -= self.learning_rate * (self.grads[f\"db{l}\"])\n",
    "\n",
    "    def train(self, dims, learning_rate = 0.01, iterations = 1000, adam_optimizer=False):\n",
    "        if iterations > 100:\n",
    "            printing_interval = round(iterations * 0.01)\n",
    "        else:\n",
    "            printing_interval = 1\n",
    "        self.initialize_parameters(dims, adam_optimizer=adam_optimizer)\n",
    "        costs = []\n",
    "        for i in range(iterations):\n",
    "            self.propagate(self.X_train)\n",
    "            cost = self.compute_cost()\n",
    "            if i % printing_interval == 0:\n",
    "                print(f\"epoch {i} : {cost}\")\n",
    "            costs.append(cost)\n",
    "            self.backprop()\n",
    "            if adam_optimizer:\n",
    "                self.update_grads_adam(t=i+1, learning_rate=learning_rate)\n",
    "            else:\n",
    "                self.update_grads_gd(learning_rate = learning_rate)\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val , y_train, y_val = generate_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : 2.0822960619623117\n",
      "epoch 4 : 1.562271564595699\n",
      "epoch 8 : 1.1647866878601822\n",
      "epoch 12 : 0.8721443738090497\n",
      "epoch 16 : 0.6592282948196263\n",
      "epoch 20 : 0.5053467408211979\n",
      "epoch 24 : 0.39570660192460255\n",
      "epoch 28 : 0.3176467161044149\n",
      "epoch 32 : 0.26141312080595236\n",
      "epoch 36 : 0.2198849983299908\n",
      "epoch 40 : 0.18853779806225593\n",
      "epoch 44 : 0.1644752363877394\n",
      "epoch 48 : 0.1456580707168918\n",
      "epoch 52 : 0.13066083952444693\n",
      "epoch 56 : 0.11845456320716033\n",
      "epoch 60 : 0.10833297744665674\n",
      "epoch 64 : 0.09979870271547937\n",
      "epoch 68 : 0.0925002345159666\n",
      "epoch 72 : 0.08618017090438514\n",
      "epoch 76 : 0.08064507412744558\n",
      "epoch 80 : 0.07574921038784457\n",
      "epoch 84 : 0.07138108839399522\n",
      "epoch 88 : 0.06745455223435247\n",
      "epoch 92 : 0.06390230872906333\n",
      "epoch 96 : 0.060671090909340204\n",
      "epoch 100 : 0.057717840517462135\n",
      "epoch 104 : 0.05500730771828038\n",
      "epoch 108 : 0.05251034252735541\n",
      "epoch 112 : 0.050202528767373404\n",
      "epoch 116 : 0.04806321737290407\n",
      "epoch 120 : 0.04607480885574349\n",
      "epoch 124 : 0.04422217534732521\n",
      "epoch 128 : 0.042492210683962826\n",
      "epoch 132 : 0.040873483224076766\n",
      "epoch 136 : 0.039355963635538845\n",
      "epoch 140 : 0.03793080495247245\n",
      "epoch 144 : 0.03659016610707677\n",
      "epoch 148 : 0.035327068770504\n",
      "epoch 152 : 0.034135278590307606\n",
      "epoch 156 : 0.03300920645969598\n",
      "epoch 160 : 0.031943825882565335\n",
      "epoch 164 : 0.030934603029799335\n",
      "epoch 168 : 0.029977437273888158\n",
      "epoch 172 : 0.029068610440003015\n",
      "epoch 176 : 0.0282047431806833\n",
      "epoch 180 : 0.02738275729870176\n",
      "epoch 184 : 0.026599843062824056\n",
      "epoch 188 : 0.025853430729812436\n",
      "epoch 192 : 0.025141165651753578\n",
      "epoch 196 : 0.024460886435515757\n",
      "epoch 200 : 0.02381060571198552\n",
      "epoch 204 : 0.02318849315798818\n",
      "epoch 208 : 0.02259286046817763\n",
      "epoch 212 : 0.022022148015945788\n",
      "epoch 216 : 0.02147491298374814\n",
      "epoch 220 : 0.020949818777757832\n",
      "epoch 224 : 0.020445625567174316\n",
      "epoch 228 : 0.019961181810621814\n",
      "epoch 232 : 0.019495416651612238\n",
      "epoch 236 : 0.019047333080955995\n",
      "epoch 240 : 0.01861600177746111\n",
      "epoch 244 : 0.01820055554989791\n",
      "epoch 248 : 0.017800184313077467\n",
      "epoch 252 : 0.017414130539316536\n",
      "epoch 256 : 0.017041685133836037\n",
      "epoch 260 : 0.016682183688919994\n",
      "epoch 264 : 0.0163350030770704\n",
      "epoch 268 : 0.015999558348080346\n",
      "epoch 272 : 0.01567529989903238\n",
      "epoch 276 : 0.015361710889784642\n",
      "epoch 280 : 0.015058304879604491\n",
      "epoch 284 : 0.014764623663319383\n",
      "epoch 288 : 0.014480235287732233\n",
      "epoch 292 : 0.014204732231136039\n",
      "epoch 296 : 0.01393772973059832\n",
      "epoch 300 : 0.01367886424330449\n",
      "epoch 304 : 0.013427792029678155\n",
      "epoch 308 : 0.01318418784726033\n",
      "epoch 312 : 0.012947743745449129\n",
      "epoch 316 : 0.012718167952195207\n",
      "epoch 320 : 0.012495183844631287\n",
      "epoch 324 : 0.012278528996400301\n",
      "epoch 328 : 0.012067954295147228\n",
      "epoch 332 : 0.011863223124265226\n",
      "epoch 336 : 0.011664110603545894\n",
      "epoch 340 : 0.011470402883883873\n",
      "epoch 344 : 0.011281896491634755\n",
      "epoch 348 : 0.011098397718627796\n",
      "epoch 352 : 0.01091972205419684\n",
      "epoch 356 : 0.010745693655918268\n",
      "epoch 360 : 0.010576144856038068\n",
      "epoch 364 : 0.010410915700834598\n",
      "epoch 368 : 0.010249853520402289\n",
      "epoch 372 : 0.010092812526557427\n",
      "epoch 376 : 0.009939653436762346\n",
      "epoch 380 : 0.009790243122141343\n",
      "epoch 384 : 0.009644454277821927\n",
      "epoch 388 : 0.009502165113980686\n",
      "epoch 392 : 0.009363259066105294\n",
      "epoch 396 : 0.00922762452310449\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neural_network.train([X_train.shape[0], y_train.shape[0]], iterations=400, learning_rate=0.1, adam_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : 2.0794728652942873\n",
      "epoch 40 : 2.078919856063855\n",
      "epoch 80 : 2.078667586252647\n",
      "epoch 120 : 2.0785207667056063\n",
      "epoch 160 : 2.07840528661267\n",
      "epoch 200 : 2.0782902707687403\n",
      "epoch 240 : 2.078159131478085\n",
      "epoch 280 : 2.0779984089410566\n",
      "epoch 320 : 2.077792982667329\n",
      "epoch 360 : 2.07752157172426\n",
      "epoch 400 : 2.0771438692832396\n",
      "epoch 440 : 2.0765879813538954\n",
      "epoch 480 : 2.0757258648689674\n",
      "epoch 520 : 2.0742983734336558\n",
      "epoch 560 : 2.0717038095271607\n",
      "epoch 600 : 2.066429292575302\n",
      "epoch 640 : 2.054069546671953\n",
      "epoch 680 : 2.0211522336097874\n",
      "epoch 720 : 1.9420410004056543\n",
      "epoch 760 : 1.7859745083801384\n",
      "epoch 800 : 1.4726056232435702\n",
      "epoch 840 : 1.1396803772741235\n",
      "epoch 880 : 0.904200765803087\n",
      "epoch 920 : 0.7383406961452554\n",
      "epoch 960 : 0.6120821208085196\n",
      "epoch 1000 : 0.7682731128247161\n",
      "epoch 1040 : 0.5008986996776318\n",
      "epoch 1080 : 0.2857605731744577\n",
      "epoch 1120 : 0.16750987273309798\n",
      "epoch 1160 : 0.11798100469807764\n",
      "epoch 1200 : 0.08906606956942333\n",
      "epoch 1240 : 0.07100247285324\n",
      "epoch 1280 : 0.05884362241410444\n",
      "epoch 1320 : 0.05014391792489629\n",
      "epoch 1360 : 0.04361835738656155\n",
      "epoch 1400 : 0.03854329345292914\n",
      "epoch 1440 : 0.034484655029682024\n",
      "epoch 1480 : 0.03116724616177017\n",
      "epoch 1520 : 0.028408045961767558\n",
      "epoch 1560 : 0.026080275707001086\n",
      "epoch 1600 : 0.02409305020566546\n",
      "epoch 1640 : 0.022379313692486323\n",
      "epoch 1680 : 0.02088838623507229\n",
      "epoch 1720 : 0.019581185849515986\n",
      "epoch 1760 : 0.018427062292891483\n",
      "epoch 1800 : 0.017401634659744317\n",
      "epoch 1840 : 0.016485273901768724\n",
      "epoch 1880 : 0.0156620119640112\n",
      "epoch 1920 : 0.014918740894055755\n",
      "epoch 1960 : 0.014244614003038928\n",
      "epoch 2000 : 0.013630590985617091\n",
      "epoch 2040 : 0.013069087600804646\n",
      "epoch 2080 : 0.012553702816024791\n",
      "epoch 2120 : 0.012079003947601127\n",
      "epoch 2160 : 0.011640356008110827\n",
      "epoch 2200 : 0.011233785061958492\n",
      "epoch 2240 : 0.010855868024030147\n",
      "epoch 2280 : 0.010503643205737141\n",
      "epoch 2320 : 0.010174537269686538\n",
      "epoch 2360 : 0.00986630525535468\n",
      "epoch 2400 : 0.009576981084444772\n",
      "epoch 2440 : 0.009304836518818225\n",
      "epoch 2480 : 0.009048346972632942\n",
      "epoch 2520 : 0.008806162911360362\n",
      "epoch 2560 : 0.008577085826382634\n",
      "epoch 2600 : 0.008360047972862827\n",
      "epoch 2640 : 0.008154095217173224\n",
      "epoch 2680 : 0.007958372463256744\n",
      "epoch 2720 : 0.007772111226467273\n",
      "epoch 2760 : 0.007594619002190698\n",
      "epoch 2800 : 0.00742527014009802\n",
      "epoch 2840 : 0.007263497985352535\n",
      "epoch 2880 : 0.007108788090038334\n"
     ]
    }
   ],
   "source": [
    "neural_network.train([X_train.shape[0],8, y_train.shape[0]], iterations=4000, learning_rate=0.1, adam_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neural_network.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5, 2, 7, 2, 1, 0, 1, 4, 0, 4, 7, 6, 7, 5, 0, 6, 5, 4, 6, 2,\n",
       "       7, 5, 5, 0, 5, 6, 1, 0, 3, 6, 7, 1, 6, 2, 3, 7, 3, 2, 1, 1, 6, 7,\n",
       "       2, 7, 4, 0, 1, 0, 3, 2, 6, 1, 6, 3, 1, 3, 7, 4, 4, 3, 5, 5, 2, 3,\n",
       "       2, 0, 1, 0, 1, 5, 3, 0, 3, 4, 4, 5, 0, 4, 4, 5, 4, 0, 1, 5, 7, 0,\n",
       "       6, 6, 3, 1, 1, 1, 3, 3, 1, 5, 6, 3, 3, 0, 3, 2, 3, 2, 2, 2, 3, 0,\n",
       "       4, 1, 2, 6, 3, 5, 3, 2, 0, 6, 4, 2, 7, 4, 4, 7, 6, 1, 1, 7, 0, 4,\n",
       "       5, 3, 3, 2, 7, 2, 0, 3, 7, 3, 0, 7, 3, 6, 2, 0, 5, 1, 0, 4, 3, 3,\n",
       "       7, 0, 1, 6, 0, 7, 7, 5, 1, 2, 4, 6, 1, 2, 1, 1, 5, 3, 6, 6, 0, 1,\n",
       "       4, 6, 4, 1, 6, 3, 2, 6, 3, 2, 7, 5, 1, 5, 4, 2, 3, 6, 1, 6, 5, 6,\n",
       "       0, 4, 4, 2, 7, 2, 4, 5, 1, 4, 4, 1, 5, 0, 2, 0, 6, 3, 7, 0, 6, 4,\n",
       "       5, 3, 1, 6, 3, 0, 5, 6, 4, 5, 3, 0, 1, 6, 2, 0, 6, 3, 7, 0, 2, 2,\n",
       "       4, 6, 0, 5, 7, 7, 0, 5, 4, 6, 5, 3, 4, 2, 4, 0, 4, 5, 3, 0, 2, 5,\n",
       "       3, 4, 5, 5, 4, 6, 0, 6, 6, 5, 1, 2, 2, 6, 1, 1, 6, 6, 7, 2, 0, 2,\n",
       "       5, 2, 4, 7, 4, 0, 4, 4, 6, 5, 0, 2, 0, 1, 2, 4, 0, 6, 6, 3, 6, 2,\n",
       "       0, 5, 1, 5, 1, 0, 4, 1, 3, 5, 2, 5, 6, 6, 6, 2, 7, 3, 0, 0, 2, 6,\n",
       "       0, 5, 5, 3, 6, 5, 0, 4, 4, 0, 2, 1, 3, 4, 1, 2, 7, 4, 0, 7, 5, 5,\n",
       "       3, 0, 3, 6, 2, 5, 4, 3, 6, 5, 2, 0, 5, 7, 5, 5, 3, 2, 5, 7, 7, 1,\n",
       "       3, 5, 5, 4, 5, 7, 6, 3, 6, 0, 3, 0, 3, 5, 3, 2, 4, 3, 1, 2, 6, 2,\n",
       "       5, 5, 2, 0, 6, 2, 0, 5, 7, 6, 1, 6, 2, 5, 0, 7, 4, 4, 6, 0, 5, 0,\n",
       "       7, 0, 2, 5, 7, 3, 0, 6, 4, 0, 7, 7, 7, 4, 3, 0, 3, 5, 0, 6, 7, 7,\n",
       "       1, 7, 2, 1, 1, 7, 3, 3, 7, 0, 4, 6, 4, 5, 6, 7, 1, 5, 5, 7, 7, 2,\n",
       "       1, 5, 5, 4, 5, 3, 3, 1, 0, 4, 6, 2, 1, 3, 5, 1, 7, 1, 2, 7, 4, 1,\n",
       "       4, 7, 4, 0, 3, 0, 2, 6, 6, 4, 7, 3, 3, 3, 4, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5, 2, 7, 2, 1, 0, 1, 4, 0, 4, 7, 6, 7, 5, 0, 6, 5, 4, 6, 2,\n",
       "       7, 5, 5, 0, 5, 6, 1, 0, 3, 6, 7, 1, 6, 2, 3, 7, 3, 2, 1, 1, 6, 7,\n",
       "       2, 7, 4, 0, 1, 0, 3, 2, 6, 1, 6, 3, 1, 3, 7, 4, 4, 3, 5, 5, 2, 3,\n",
       "       2, 0, 1, 0, 1, 5, 3, 0, 3, 4, 4, 5, 0, 4, 4, 5, 4, 0, 1, 5, 7, 0,\n",
       "       6, 6, 3, 1, 1, 1, 3, 3, 1, 5, 6, 3, 3, 0, 3, 2, 3, 2, 2, 2, 3, 0,\n",
       "       4, 1, 2, 6, 3, 5, 3, 2, 0, 6, 4, 2, 7, 4, 4, 7, 6, 1, 1, 7, 0, 4,\n",
       "       5, 3, 3, 2, 7, 2, 0, 3, 7, 3, 0, 7, 3, 6, 2, 0, 5, 1, 0, 4, 3, 3,\n",
       "       7, 0, 1, 6, 0, 7, 7, 5, 1, 2, 4, 6, 1, 2, 1, 1, 5, 3, 6, 6, 0, 1,\n",
       "       4, 6, 4, 1, 6, 3, 2, 6, 3, 2, 7, 5, 1, 5, 4, 2, 3, 6, 1, 6, 5, 6,\n",
       "       0, 4, 4, 2, 7, 2, 4, 5, 1, 4, 4, 1, 5, 0, 2, 0, 6, 3, 7, 0, 6, 4,\n",
       "       5, 3, 1, 6, 3, 0, 5, 6, 4, 5, 3, 0, 1, 6, 2, 0, 6, 3, 7, 0, 2, 2,\n",
       "       4, 6, 0, 5, 7, 7, 0, 5, 4, 6, 5, 3, 4, 2, 4, 0, 4, 5, 3, 0, 2, 5,\n",
       "       3, 4, 5, 5, 4, 6, 0, 6, 6, 5, 1, 2, 2, 6, 1, 1, 6, 6, 7, 2, 0, 2,\n",
       "       5, 2, 4, 7, 4, 0, 4, 4, 6, 5, 0, 2, 0, 1, 2, 4, 0, 6, 6, 3, 6, 2,\n",
       "       0, 5, 1, 5, 1, 0, 4, 1, 3, 5, 2, 5, 6, 6, 6, 2, 7, 3, 0, 0, 2, 6,\n",
       "       0, 5, 5, 3, 6, 5, 0, 4, 4, 0, 2, 1, 3, 4, 1, 2, 7, 4, 0, 7, 5, 5,\n",
       "       3, 0, 3, 6, 2, 5, 4, 3, 6, 5, 2, 0, 5, 7, 5, 5, 3, 2, 5, 7, 7, 1,\n",
       "       3, 5, 5, 4, 5, 7, 6, 3, 6, 0, 3, 0, 3, 5, 3, 2, 4, 3, 1, 2, 6, 2,\n",
       "       5, 5, 2, 0, 6, 2, 0, 5, 7, 6, 1, 6, 2, 5, 0, 7, 4, 4, 6, 0, 5, 0,\n",
       "       7, 0, 2, 5, 7, 3, 0, 6, 4, 0, 7, 7, 7, 4, 3, 0, 3, 5, 0, 6, 7, 7,\n",
       "       1, 7, 2, 1, 1, 7, 3, 3, 7, 0, 4, 6, 4, 5, 6, 7, 1, 5, 5, 7, 7, 2,\n",
       "       1, 5, 5, 4, 5, 3, 3, 1, 0, 4, 6, 2, 1, 3, 5, 1, 7, 1, 2, 7, 4, 1,\n",
       "       4, 7, 4, 0, 3, 0, 2, 6, 6, 4, 7, 3, 3, 3, 4, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_val.T, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(y_pred.T, axis = 1), np.argmax(y_val.T, axis = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
